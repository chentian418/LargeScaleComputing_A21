{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-responsibility",
   "metadata": {},
   "source": [
    "## Introduction to Boto3 and AWS Serverless Solutions\n",
    "\n",
    "Let's say that we wanted to detect objects in an image, extract text from images, or perform sentiment analysis on a text. We could write and train our own classifiers, run our classifier on a server (e.g. an EC2 instance) and use this to make predictions. This requires a lot of time and energy in selecting the appropriate hardware, software, techniques, etc. necessary to perform these operations.\n",
    "\n",
    "For this reason, all the major cloud providers offer serverless \"functions as a service\" which are pre-trained/coded models that you simply need to provide data to and you will receive a response. Your cloud provider (e.g. AWS) will spin up the compute instances necessary to actually run the code. \n",
    "\n",
    "You can access all of these through the AWS Console, but it is easier to integrate them into your existing code by via the Boto3 SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "backed-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-costume",
   "metadata": {},
   "source": [
    "For instance, we can interact with AWS' image recognition functions like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "therapeutic-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "rekog = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "national-jaguar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Person', 99.89472198486328),\n",
       " ('Human', 99.89472198486328),\n",
       " ('Outdoors', 94.6645278930664),\n",
       " ('Road', 94.50630187988281),\n",
       " ('Path', 94.26421356201172)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect the objects in the provided image\n",
    "with open('uchicago.jpg', 'rb') as image:\n",
    "    response = rekog.detect_labels(Image={'Bytes': image.read()})\n",
    "    \n",
    "[(label['Name'], label['Confidence']) for label in response['Labels']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "encouraging-participant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also count number of instances of each label: e.g. \"Person\" - label 0\n",
    "len(response['Labels'][0]['Instances']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-rabbit",
   "metadata": {},
   "source": [
    "We can use rekognition to detect text in images as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "falling-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uchicago_sign.jpg', 'rb') as image:\n",
    "    response = rekog.detect_text(Image={'Bytes': image.read()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interstate-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text:THE UNIVERSITY OF\n",
      "Confidence: 100.00%\n",
      "Detected text:CHICAGO\n",
      "Confidence: 97.42%\n"
     ]
    }
   ],
   "source": [
    "for text in response['TextDetections']:\n",
    "    if text['Type'] == 'LINE':\n",
    "        print ('Detected text:' + text['DetectedText'])\n",
    "        print ('Confidence: ' + \"{:.2f}\".format(text['Confidence']) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-nightmare",
   "metadata": {},
   "source": [
    "If you have custom workflows, Rekognition might not be the best option, but for many general applications, this will likely handle everything that you need to do and is really easy to use.\n",
    "\n",
    "We can also perform common NLP tasks like detecting the sentiment of a text via AWS Comprehend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "knowing-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE {'Positive': 0.9994707703590393, 'Negative': 4.922328662360087e-05, 'Neutral': 0.00045435165520757437, 'Mixed': 2.5665714929345995e-05}\n"
     ]
    }
   ],
   "source": [
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "response = comprehend.detect_sentiment(Text='This class is fun!',\n",
    "                                       LanguageCode='en')\n",
    "\n",
    "print(response['Sentiment'], response['SentimentScore'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-employment",
   "metadata": {},
   "source": [
    "...and perform quick translations from one language (here, automatically detected) into another one (French) on command with AWS Translate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "earlier-software",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour, je m'appelle Jon\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = boto3.client('translate')\n",
    "\n",
    "response = translate.translate_text(Text='Hello, my name is Jon',\n",
    "                                    SourceLanguageCode='auto',\n",
    "                                    TargetLanguageCode='fr')\n",
    "response['TranslatedText']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-chess",
   "metadata": {},
   "source": [
    "Will have a chance to practice using more of these serverless tools in the DataCamp course that we've assigned as one of the readings for Monday's class, but this should give you a taste of some of the functionality that is available to you right out of the box.\n",
    "\n",
    "----\n",
    "\n",
    "**AWS Lambda Functions**\n",
    "\n",
    "We can also create our own custom serverless functions as well, though, via AWS Lambda... \n",
    "\n",
    "*Go to AWS Console and create/deploy sample Lambda function (called `HelloWorld`):*\n",
    "\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    # test: {'key1': 1, 'key2': 2}\n",
    "    total = event['key1'] + event['key2']\n",
    "    return total\n",
    "```\n",
    "\n",
    "Can write code of arbitrary complexity in here, assuming it's going to be a relatively quick operation (e.g. less than 300s)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rising-miniature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_lambda = boto3.client('lambda')\n",
    "\n",
    "test_data = {'key1': 1, 'key2': 2}\n",
    "\n",
    "# run synchronously:\n",
    "r = aws_lambda.invoke(FunctionName='HelloWorld',\n",
    "                      InvocationType='RequestResponse',\n",
    "                      Payload=json.dumps(test_data))\n",
    "json.loads(r['Payload'].read()) # print out response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-organizer",
   "metadata": {},
   "source": [
    "Currently still running all of this code serially, though. Real advantage of\n",
    "Lambda is that it scales automatically to meet concurrent demand, meaning\n",
    "that it will automatically parallelize based on how many concurrent invocations\n",
    "it receives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "coordinate-titanium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. write function to invoke our function for us and pass in data:\n",
    "def invoke_function(data):\n",
    "    r = aws_lambda.invoke(FunctionName='HelloWorld',\n",
    "                       InvocationType='RequestResponse',\n",
    "                       Payload=json.dumps(data))\n",
    "    return json.loads(r['Payload'].read())\n",
    "\n",
    "# 2. Demo that lambda function will scale out if called concurrently on different threads locally\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = executor.map(invoke_function, [test_data for _ in range(4)])\n",
    "\n",
    "# 3. In AWS Console: confirm that we had four concurrent executions (takes a few seconds to update)\n",
    "# Same results too:\n",
    "[result for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-joining",
   "metadata": {},
   "source": [
    "Ideally, we should be able to scale out to as many available Lambda workers as possible (i.e. thousands of concurrent function invocations on different segments of a dataset -- a serverless domain decomposition) and not be limited by our local resources, though. This is a where a package like `pywren` can be useful in helping us spread work across many Lambda workers at once with very little code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
